# -*- coding: utf-8 -*-
"""neural_networks_and_deep_learning_recap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fCHTaH6p85LlpheyrbVguYmkhVfoGK_e

# Neural Networks and Deep Learning Recap

useful links: 

https://blog.keras.io/on-the-importance-of-democratizing-artificial-intelligence.html

## What's the difference between Artificial Intelligence, Machine Learning and Deep Learning?

**Artificial Intelligence** (AI) is a very old field of research. It started in the 1950s; at that time, the pioneers in AI were convinced that they could make machines think. The initial approach was attempting to model intellectual tasks, and program those tasks in computers. That is, in the beginning, AI was about people *hard-coding* problems that the machine tried to solve. A good example of an early AI program is computer chess. Early AI computer chess programs had **lots of rules pre-conceived by the program designers**. So although the term refers to "intelligence", these programs were not "intelligent", but just a sophisticated set of rules.

* The term *neural network* was also born at that time. The initial idea was trying to mimic how a *neuron* works (or in fact, how it was thought a neuron worked when the term was coined) in the brain. As in the case of *Artificial Intelligence*, please do not be fooled by the terms. A *neural network* is just a way to compute approximation to functions. A powerful and useful way, true. But it is not any kind of "artificial brain", or anything like that.

This approach is now obsolete. We now know that some tasks are so complex, that we will never be able to pre-design a system to have a good performance on those tasks. Take the example of the game of Go. Any program designed by humans to play Go was dull and could not compete against professional players. The game is so hard that is just impossible for us as humans to conceived a good Go computer player.
* https://en.wikipedia.org/wiki/Go_(game)

The modern approach to solve this kind of problems involves the use of data. This is what the *learning* part of *Deep Learning* (and *Machine Learning*) refers to. **Learning from data**. So, with this brief introduction, we can say that *Artificial Intelligence* is a broad concept that includes *Machine Learning*, that includes *Deep Learning*. *Artificial Intelligence* does not necessarily means learning from data, but Machine Learning does. And *Deep Learning* is just doing *Machine Learning* with neural networks.

![](imgs/01_ai_ml_dl.png)

We will see during this course what *learning* means, and what is a *neural network*. 

In the case of the game of Go, all attempts to design a good Go computer player failed, until Google DeepMind created AlphaGo, a system that *learned* to play Go based on data from previous games. AlphaGo is an example that learning from data can provide computer systems that are superior to any system pre-conceived by humans.
* Find out more about AlphaGo by watching this utterly interesting movie: https://www.alphagomovie.com/

In this lesson, we will give our first steps on *Deep Learning* and *Neural Networks*, to start creating systems that perform complex tasks by learning from data. Instead of attempting to derive the rules by ourselves, those rules will be inferred by a neural network based on the data we provide:

![](imgs/02_rules_data.png)

# Deep Learning and Machine Learning: why is everyone now talking about Deep Learning?

To create our system, we need data:
* Input data points; e.g. images to recognize shapes inside the images.
* Examples of expected output. The system will use these examples to learn from them. For the images above, we will need *labels* telling us what is in the image.

The neural network will *learn* from this data. But in fact, what the network is doing is learning a *representation* of the data that makes the task much easier to solve.

Let's see an example. We have the following input data (x,y coordinates for the points), and the labels of each item (the color of the point). We would like to obtain a model to separete the two classes of points:

![](imgs/03_points.png)

How would you separate the points? In the current reference system, it can be a complex task. But if you change the reference system (*if you learn a new representation of the data*), the task becomes trivial:

![](imgs/04_representation.png)

In Deep Learning, we learn **layers** of increasingly better representations. The **deep** in Deep Learning refers to the number of layers that we learn. Before Deep Learning, the models that we are going to see today were much shallower. The most typical form of the layer representations is **neural networks**. As we will see, **neural networks** are just chain of matrix operations -- arrays of 2 dimensions--, or more precisely, tensor operations, we will operate in dimensions higher than 2.


* More about learning representations: https://www.bbvadata.com/not-deep-learning-learning-represent/
"""

import numpy as np

x = np.array([1, 2])
W = np.array([[-1, 5], 
              [1, 2]])
b = np.array([3, -3])
layer = W.dot(x)+b
layer_with_activation = np.maximum(layer, 0.0)
layer, layer_with_activation

"""## Neural Networks

A neural network is just a stacked set of layers. Each layer is represented by a tensor, and it performs an affine transformation on each one of the layers.
* A matrix is a 2D tensor
* Affine transformation $\vec{y} = W\cdot\vec{x} + \vec{b}$, where $W$ is the weights tensor, $\vec{x}$ is the input data, $\vec{y}$ is the output data and $\vec{b}$ is called the bias
* $W$ and $\vec{b}$ are model parameters that need to be found by training

![](imgs/05_nn.png)

Note that this is a **linear operation**, therefore the result will be linear too. But the functions we want to learn are normally non-linear. How do we overcome this limiation? By using an **activation function**, that transforms the result from the linear operation. So, we actually do the following operation:

$\vec{y} = f\left(W\cdot\vec{x} + \vec{b}\right)$

There are several common activation functions, depending on the characteristics of the data and the kind of problem we want to solve. Here we will use the probably most often used activation function: **rectified linear unit**, or **relu** for friends:

![Relu plot](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)

(image extracted from https://en.wikipedia.org/wiki/Rectifier_(neural_networks))

Relu will add the non-linearity that we need to learn arbitrary functions.

Softplus is an activation function. It can be viewed as a smooth version of ReLU. 

We can achieve the same with many other activation functions. See the documentation of Keras for details on the available activation functions:
 - https://keras.io/activations/


Each layer learns a new representation of the input data, and after several layers, the representation will make it straightforward to solve the task (e.g. assiging a digit to an image of a digit).

Until very recently, any attempt to train a deep neural network would not provide good results. Recent progresses (e.g. advances in hardware, new activation functions) make it possible to train very deep network, and therefore, solve problems that were unfeasible not long ago.

### Training process

The parameters of the network are initially randomly assigned. Then after each pass of the network, that is, after each *epoch*, the predictions of the network is compared against the true values, applying a *loss function*. This score is used to update the parameters of the network, with a process called **backpropagation**.
* Backpropagation is a complex process that involves calculating the gradient of the parameters of the network. For more details see: https://en.wikipedia.org/wiki/Backpropagation
* See also this excellent and visual explanation of how backpropagation works: https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/

The training process is just an optimization process, that is normally done using a gradient descent-based algorithm. The values of gradients are computed using the backpropagation algorithm. Each epoch will make the loss score to be lower, and the predictions to be better.

![](imgs/06_optim.png)

In practice, we have to fine tune the number of epochs (and other parameters), until we obtain a result that is good enough for our purpose. In fact, if the number of epochs is too high, our model may **overfit** the training data. In that case, it will perform worsely when the model is exposed to data that it  has not seen previously.
* Fine tuning the number of epochs, and other parameters (e.g. the number and shape of the layers) is known as **hyperparameters optimization**, or **hyperparameters tuning**
* The parameters of the model are the $W$ and $b$ values of each layer, and those are determined by training.

## Hyperparameters tuning

In summary, when we are trying to obtain a deep learning model, the process that we will follow is:
* Decide on the architecture of the network (number of layers, type of each layer, size of each layer)
* Decide on the activation functions on each layer
* Decide on the parameters related to the optimization process (e.g. learning rate)
* Decide on the loss function that we will use
* Decide on the metrics that we will use to evaluate the performance of the model
* Decide on the training parameters (number of epochs, batch size)
* Decide on how to split the data between training, validation and test sets.

We obtain feedback for those decisions with the results of the training applied to the validation set. We train once, change some of the hyperparameters, train again, decide if that's better, and we keep going until we have found a model that is satisfactory enough for our purposes.

After that, we finally evaluate the model on the test set.

## Our first neural network

Let's start with an easy example. We are going to produce some synthetic data, and will try to predict the function that generates the data. We will add some noise, to make the problem more difficult.

The function we are going to try to predict is

$\displaystyle y = 3*x + 2$
"""

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
plt.style.use('seaborn-talk')

def plot_metric(history, metric):
    history_dict = history.history
    values = history_dict[metric]
    if 'val_' + metric in history_dict.keys():  
        val_values = history_dict['val_' + metric]

    epochs = range(1, len(values) + 1)

    if 'val_' + metric in history_dict.keys():  
        plt.plot(epochs, val_values, label='Validation')
    plt.semilogy(epochs, values, label='Training')

    if 'val_' + metric in history_dict.keys():  
        plt.title('Training and validation %s' % metric)
    else:
        plt.title('Training %s' % metric)
    plt.xlabel('Epochs')
    plt.ylabel(metric.capitalize())
    plt.legend()

    plt.show()

"""### Data Preparation"""

x_train = np.linspace(0, 100, 1000)
y_train = 3*x_train + 2

plot(x_train, y_train)

x_test = np.linspace(100.01, 110, 500)
y_test = 3*x_test + 2

plt.plot(x_train, y_train, 'b')
plt.plot(x_test, y_test,'r')
plt.show()

import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow.keras as keras

from keras import layers

"""y = relu(W*x +b)"""

layers.Dense(units=64, activation='relu')

from keras import Sequential

model = Sequential()

"""We have x, y as joint distribution of data and y as labelsy. 
We need to build a model with 128, 64 and 32 neurons (or units or nodes). 
"""

model = Sequential()

capa1 = layers.Dense(units=128, activation='relu', input_shape=(1, ))
model.add(capa1)
capa2 = layers.Dense(units=64, activation='relu')
model.add(capa2)
model.add(layers.Dense(units=32, activation='relu'))
model.add(layers.Dense(units=1, activation='relu'))

model.summary()

"""The number of parameters is equal to the number of weight + biases

[parameters] = W*x +b

[W] = m x n 

Example 1:

[x] = 1, [b] = 128, [W] = 128 x 1  => 128+128=256

Example 2:

[x] = 128, [b] = 64, [W] = 64 x 128  => 8192+64 = 8256

Example 3:

[x] = 64, [b] = 32, [W] = 32 x 64  => 2048+32 = 2080

Example 4:

[x] = 32, [b] = 1, [W] = 1 x 32  => 32+1 = 33

"""

64*32

model.summary()

from tensorflow.keras import optimizers
from tensorflow.keras import losses
from tensorflow.keras import metrics

model.compile(optimizer=optimizers.RMSprop(), loss=losses.mse)

x_train.shape[0] // 100

h = model.fit(x=x_train, y=y_train, batch_size=100, epochs=20, validation_split=0.1, verbose = 2)

plot_metric(h, 'loss')

# build model
model = Sequential()

capa1 = layers.Dense(units=128, activation='relu', input_shape=(1, ))
model.add(capa1)
capa2 = layers.Dense(units=64, activation='relu')
model.add(capa2)
model.add(layers.Dense(units=32, activation='relu'))
model.add(layers.Dense(units=1, activation='relu'))

model.summary()

# compile
model.compile(optimizer=optimizers.RMSprop(), loss=losses.mse, metrics=[metrics.mean_absolute_error])

# fit
h = model.fit(x=x_train, y=y_train, batch_size=100, epochs=20, validation_split=0.1)
plot_metric(h, 'loss')

plot_metric(h, 'mean_absolute_error')

# data normalization
max_x, min_x = x_train.max(), x_train.min()
x_train_norm = (x_train - min_x) / (max_x - min_x)

max_y, min_y = y_train.max(), y_train.min()
y_train_norm = (y_train - min_y) / (max_y - min_y)

x_test_norm = (x_test - min_x) / (max_x - min_x)
y_test_norm = (y_test - min_y) / (max_y - min_y)

plot(x_train_norm, y_train_norm, 'b')
plot(x_test_norm, y_test_norm, 'r')

# build model
m = Sequential()
m.add(layers.Dense(32, activation='relu', input_shape=(1, )))
m.add(layers.Dense(16, activation='relu'))
m.add(layers.Dense(1, activation='relu'))

m.summary()
# compile 
m.compile(optimizer=optimizers.RMSprop(), loss=losses.mean_squared_error, metrics=[metrics.mean_absolute_error])

# fit
h = m.fit(x_train_norm, y_train_norm, batch_size=100, epochs=30, validation_split=0.1)

plot_metric(h, 'loss')
plot_metric(h, 'mean_absolute_error')

p = m.predict(x_test_norm)
#plt.plot(x_train_norm, y_train_norm, 'b')
#plt.plot(x_test_norm, y_test_norm, 'r')
plt.plot(x_test_norm, p, '--k')

# build model
m = Sequential()
m.add(layers.Dense(10, activation='relu', input_shape=(1, )))
m.add(layers.Dense(4, activation='relu'))
m.add(layers.Dense(1, activation='relu'))

m.summary()
# compile 
m.compile(optimizer=optimizers.RMSprop(), loss=losses.mean_squared_error, metrics=[metrics.mean_absolute_error])

# fit
h = m.fit(x_train_norm, y_train_norm, batch_size=100, epochs=10, validation_split=0.1)

plot_metric(h, 'loss')
plot_metric(h, 'mean_absolute_error')

p = m.predict(x_test_norm)
plt.plot(x_train_norm, y_train_norm, 'b')
plt.plot(x_test_norm, y_test_norm, 'r')
plt.plot(x_test_norm, p, '--k')

"""# HOMEWORKS

### Now another more complex function
Try convolutions if you want

Let's try to fit a parabollic function

$\displaystyle y = -2x^2 + x + 1$
"""



"""### And now even a more complex one
Try convolutions if you want

Let's try to predict now this function:

$\displaystyle y = \cos(x)$
"""

