{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random_neural_networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1xhdzk6PF0u"
      },
      "outputs": [],
      "source": [
        "# general imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "\n",
        "#keras imports\n",
        "#import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "import warnings\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "input_dim = 784                                                     # set the dimensionality of data such as mnist 28 x 28 images\n",
        "\n",
        "model = Sequential()                                                # Initialize Keras model as Sequential since this is a simple model (in keras there are 2 types of model: Sequential and Model for more complicated models    \n",
        "model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
        "model.add(Dense(10, activation='softmax'))                          # softmax is actually a normalization e^y/sum(e^y) which produces probabilities\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "              \n",
        "# Generate dummy data\n",
        "data = np.random.randint(255, size=(60000, input_dim))\n",
        "labels = np.random.randint(10, size=(60000, 10))\n",
        "\n",
        "#(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalization\n",
        "data = data / 255.0                                                  # for faster convergece of sgd\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "#labels = to_categorical(labels, num_classes=10)                      # translate to a matrix 60000 x 10, with 5th column = 1 if the label is 5, the 8th col = 1 if label = 8, otherwise is 0\n",
        "                                                                     # this is because: output from neural network is a vector of 10 probabilities whereas label vectors is made of scalars (single values), for example 5,4,2,8\n",
        "                                                                     # in order to calculate derivatives we have to translate labels vector in one-hot encoding: 8 becomes [0,0,0,0,0,0,0,1,0,0], this is what to_categorical does\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, labels, validation_split = 0.2, epochs=10, batch_size=32)       "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Home Work: Run this program on Mnist and check accuracy. Why accuracy now is different? Why accuracy for random data is around 10% ? "
      ],
      "metadata": {
        "id": "SYVbBaD9POpk"
      }
    }
  ]
}