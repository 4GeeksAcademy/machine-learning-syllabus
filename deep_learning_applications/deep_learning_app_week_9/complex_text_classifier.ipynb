{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU0UfiwyQj0q"
      },
      "source": [
        "# **All Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtV03Ht1QiU_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Created on 28/03/2022\n",
        "\n",
        "@author: Francesco Pugliese\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import pdb\n",
        "\n",
        "# Other imports\n",
        "import numpy\n",
        "from os import listdir\n",
        "import random\n",
        "import os\n",
        "import timeit\n",
        "import platform\n",
        "import string\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqdUTisPJLIt"
      },
      "source": [
        "#**All Utility Functions Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2TUlCWSJVV6"
      },
      "outputs": [],
      "source": [
        "def data_download(file_to_download, gdrive_code, OS, uncompress = True):\n",
        "  if not os.path.exists(file_to_download):\n",
        "    os.system('gdown --id \"'+gdrive_code+'\" --output '+file_to_download)\n",
        "    if OS == \"Linux\" and uncompress:\n",
        "        os.system('unzip -o -n \"./'+file_to_download+'\" -d \"./\"')\n",
        "    return True\n",
        "  else: \n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEyxQNHh-ifd"
      },
      "source": [
        "# **All Downloads**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipm4TGgv-mE7",
        "outputId": "c00b0e71-3775-4860-9a94-88862266f216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time:  2.607292353000048\n"
          ]
        }
      ],
      "source": [
        "start_time = timeit.default_timer()\n",
        "# Operating System\n",
        "OS = platform.system()                           # returns 'Windows', 'Linux', etc\n",
        "\n",
        "os.system('pip install --upgrade --no-cache-dir gdown')\n",
        "\n",
        "out = data_download(\"./Sentiment_Analysis_Data.zip\", \"1nTPYscUe56_K40S7oDWZriyB5poN6CB3\", OS)\n",
        "out = data_download(\"./complex_classifier_libs.zip\", \"1fXl4lpnUbqO5EHyZkdr059iFAuxwUQcO\", OS)\n",
        "\n",
        "\n",
        "print(\"Elapsed Time: \", timeit.default_timer() - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQwnN5nsKSb4"
      },
      "source": [
        "# **All Program Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug7PVS13KR3R"
      },
      "outputs": [],
      "source": [
        "# Keras imports\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Preprocessing\n",
        "from Preprocessing.preprocessing import load_datasets, load_sentipolc_datasets, create_word_index, prepare_embedding_matrix\n",
        "from Embeddings.w2v_preprocessing import initialize_embeddings, encode_fast_embeddings\n",
        "\n",
        "# Training\n",
        "from Training.training import Training\n",
        "\n",
        "# Misc\n",
        "from Misc.utils import delete_empty_tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMcajjqFLcPC"
      },
      "source": [
        "#**All initializations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO1tMS2ILkgW",
        "outputId": "81da6edf-e1db-4d6e-b366-5f58a40c9be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1.2\n"
          ]
        }
      ],
      "source": [
        "os.system('pip install --upgrade gensim')\n",
        "import gensim\n",
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTnmorHWQ1ku"
      },
      "source": [
        "#**All Globals**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJXszo2tPv-E"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Operating System\n",
        "OS = platform.system()                      # returns 'Windows', 'Linux', etc\n",
        "language = 'en'\n",
        "\n",
        "# preprocessing\n",
        "fastEmbeddingsLoad = True\n",
        "fastEmbeddingsSave = False\n",
        "\n",
        "# train set\n",
        "MAX_NB_WORDS = 10000\n",
        "validation_split = 0.2\n",
        "#trainset_limit = 1000\n",
        "trainset_limit = None\n",
        "\n",
        "# Training\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "save_best_model = True\n",
        "save_log = False\n",
        "return_epochs = True\n",
        "solver = 'adadelta'\n",
        "incremental = False\n",
        "\n",
        "# Embeddings\n",
        "embedding_size = 300\n",
        "embeddings_type = 'word2vec'\t\t\t\t\t\t# Embeddings type: w2v = word2vec, glove = Glove\n",
        "\n",
        "# Initialization \n",
        "\n",
        "# Set CPU or GPU type\n",
        "gpu = True\n",
        "gpu_id = \"0\"\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "if gpu == False: \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "else: \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
        "\n",
        "sentiment_dataset_path = './Sentiment_Analysis_Data/'+language+'/trainsets/'\n",
        "\n",
        "padded_sequence_length = 140\n",
        "longest = False                      # True = considering the maximum twitters length\n",
        "\n",
        "if fastEmbeddingsLoad == False: \n",
        "    embeddings_file = 'wiki.'+language+'.vec'\n",
        "else: \n",
        "    embeddings_file = 'wiki.'+language+'.vec'+'.bin'\n",
        "\n",
        "if fastEmbeddingsSave == True:\n",
        "    embeddings_slow_file = 'wiki.'+language+'.vec'\n",
        "\n",
        "embeddings_path = './Sentiment_Analysis_Data/'+language+'/embeddings/'+embeddings_type+'/'+str(embedding_size)\n",
        "\n",
        "# Model \n",
        "#neural_model = 'recconvnet'\n",
        "neural_model = 'lstm'\n",
        "models_path = '../SavedModels/'+language\n",
        "model_file = \"best_sentiment_deep_model_windows.snn\"        \n",
        "model_file = neural_model + '_' + model_file\n",
        "\n",
        "# Testing\n",
        "save_valid_set = False\t\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv39BDD3Tigg"
      },
      "source": [
        "# **All Functions Definitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYfCjgNsTbQ2"
      },
      "outputs": [],
      "source": [
        "def train_all(models_path, model_file, x_train_txt, y_train, x_val_txt, y_val, data_set, labels, num_validation_samples, save_valid_set, embeddings_model, embedding_size, save_best_model, max_nb_words):\n",
        "       \n",
        "    if save_valid_set == True: \n",
        "        numpy.savetxt(sentiment_testset_path+'/'+\"x_validation.csv\", x_val_txt, delimiter = ',', fmt = \"%s\")\n",
        "        numpy.savetxt(sentiment_testset_path+'/'+\"y_validation.csv\", y_val, delimiter = ',')\n",
        "\n",
        "    # create word index\n",
        "    print('\\nCreating Word Index...')\n",
        "\t\n",
        "    data, word_index = create_word_index(data_set = data_set, max_words = max_nb_words, padded_sequence_length = padded_sequence_length, longest = longest)                                             # Translate textual words into indices words\n",
        "        \n",
        "    print('\\nFound %s unique words within the train set.' % len(word_index))\n",
        "    print('Shape of data tensor:', data.shape)\n",
        "    print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "    x_train = data[:-num_validation_samples]\n",
        "    x_val = data[-num_validation_samples:]\n",
        "\n",
        "    # prepare embedding magtrix\n",
        "    embedding_matrix, not_matched = prepare_embedding_matrix(word_index = word_index, max_words = max_nb_words, embeddings_model = embeddings_model, embedding_size = embedding_size)\n",
        "    print('\\n%i train-set words not found in word embeddings corpus.' % (not_matched))\n",
        "\n",
        "    \n",
        "    # Start training\n",
        "    print('Building model...')\n",
        "\n",
        "    train_start_time = timeit.default_timer()\n",
        "        \n",
        "    Training.train(x_train, y_train, x_val, y_val, neural_model, batch_size, epochs, embedding_matrix, models_path, model_file, save_best_model, solver, save_log, return_epochs, incremental, None, None)\n",
        "\n",
        "    train_end_time = timeit.default_timer()\n",
        "\n",
        "    return train_end_time - train_start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvg0TNHYUsYw"
      },
      "source": [
        "# **Body of the program**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1G0l-WiTfGL",
        "outputId": "414a48cd-9700-42ad-ee4d-2db5ab635272"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Data Sets from files CSV...\n",
            "Reading: positive_eng.txt\n",
            "800000 lines read\n",
            "Reading: negative_eng.txt\n",
            "800000 lines read\n",
            "\n",
            "Loading Word Embeddings (English Language Corpus)...\n",
            "\n",
            "\n",
            "Creating Word Index...\n",
            "\n",
            "Found 690960 unique words within the train set.\n",
            "Shape of data tensor: (1600000, 140)\n",
            "Shape of label tensor: (1600000, 1)\n",
            "\n",
            "799 train-set words not found in word embeddings corpus.\n",
            "Building model...\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 140, 300)          3000300   \n",
            "                                                                 \n",
            " sequential_2 (Sequential)   (None, 1)                 269121    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,269,421\n",
            "Trainable params: 269,121\n",
            "Non-trainable params: 3,000,300\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 140, 128)          219648    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269,121\n",
            "Trainable params: 269,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "numpy.random.seed(23455) \n",
        "global_start_time = timeit.default_timer()\n",
        "\n",
        "# Preprocessing\n",
        "if language == \"en\": \n",
        "    datasets = load_datasets(sentiment_dataset_path = sentiment_dataset_path, validation_split = validation_split, shuffle = True, limit = trainset_limit)      # Loads Trainig and Validation Set\n",
        "elif language == \"it\": \n",
        "    datasets = load_sentipolc_datasets(sentiment_dataset_path = sentiment_dataset_path, validation_split = validation_split, shuffle = True, limit = trainset_limit)      # Loads Trainig and Validation Set\n",
        "else: \n",
        "    print(\"\\nLanguage not valid: %s\" % (language))\n",
        "    sys.exit(\"\")\n",
        "\t\n",
        "x_train_txt, y_train = datasets[0] \n",
        "x_val_txt, y_val = datasets[1]\n",
        "data_set, labels = datasets[2]\n",
        "num_validation_samples = datasets[3]\n",
        "\n",
        "# Word embeddings\n",
        "if fastEmbeddingsSave == True:\n",
        "    encode_fast_embeddings(embeddings_path, embeddings_slow_file)\n",
        "\n",
        "embeddings_model = initialize_embeddings(embeddings_path, embeddings_file, fastEmbeddingsLoad, language)\n",
        "\n",
        "train_time = train_all(models_path, model_file, x_train_txt, y_train, x_val_txt, y_val, data_set, labels, num_validation_samples, save_valid_set, embeddings_model, embedding_size, save_best_model, max_nb_words = MAX_NB_WORDS)\n",
        "\t\t\t\t\t   \n",
        "print ('\\n\\nTraining time: %.2f minutes\\n' % (train_time  / 60.))\n",
        "\n",
        "end_time = timeit.default_timer()\n",
        "print ('\\n\\nGlobal time: %.2f minutes\\n' % ((end_time - global_start_time) / 60.))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework**"
      ],
      "metadata": {
        "id": "fos9MTBP5yDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Train on all the dataset (1.6 mln)\n",
        "\n",
        "2) Try all the best models accuracies and write them, make a comparison of all methods (shallow, lstm, conv1d, rec_conv_net, etc"
      ],
      "metadata": {
        "id": "gOyrOARU52ly"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "complex_text_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}