{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uk_land_registry_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Regression Problem**\n",
        "\n",
        "**Definition**: \n",
        "\n",
        "In Supervised Machine Learning there are two main tasks we can address: \n",
        "\n",
        "1) **Classification:** this refers to a predictive modeling problem where a class label is predicted for a given example of input data. Examples of classification problems include: Given an example, classify if it is spam or not. Given a handwritten character, classify it as one of the known characters. The outcome is always a **categorical variable. **\n",
        "\n",
        "2) **Regression:** this is a task for searching the relationship between independent variables (or features) and a dependent variable (or outcome). It's used as a method for predictive modelling in machine learning, in which an algorithm is used to predict **continuous variables.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mSUwNS3WKJr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Task: From an Interview exercise**"
      ],
      "metadata": {
        "id": "mH27F6bwgF_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The UK Government’s Land Registry Data is a Dataset with 22 million rows. In order to deal with this enormity of data I decided to apply deep learning models working on GPUs (Graphics Processing Unit). Specifically, I used a mixed neural network adopting convolutional layers and multi-layer perceptron layers with 1 single output. Output neuron has no activation, threshold or something, it is free to supply the estimated price value. \n",
        "Furthermore, it can be proven that convolutional (1D or 2D) with regularization techniques are key components in preventing overfitting and reaching better performances. Also, some techniques of regularization can be used to reduce model capacity while maintaining low loss. Hence, I adopted the following regularizations: \n",
        "\n",
        "**Dropout**\n",
        "\n",
        "At each training iteration a dropout layer randomly removes some neurons in the network with a certain probability. This can be seen in a two-pronged way: a) Neurons  become more independent to the weights of the other neurons (less co-adaptivity), therefore the model is more robust in order to generalize on the test set; b) dropout can be proven being a form of averaging multiple models. The “Ensemble” of models shows better performance in most machine learning tasks. \n",
        "\n",
        "**Batch Normalization**\n",
        "\n",
        "Generally, since deep neural networks adopt mini-batch training they require a long tuning of the weight initialization and learning parameters. On one side, the initialization of weights (randomly chosen) are far away from the final learned weights. On the other side, especially with deep networks,  a small perturbation in the initial layers, leads to a large change in the later layers. Batch normalization fosters the reduction of the well-known problem of \"internal covariate shift\" which causes huge perturbations of gradients and weights . By normalizing the data in each mini-batch, this problem is largely avoided.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "In order to implement a fast solution to the problem within the exercise, I made use of python libraries like Keras for the model encoding, Numpy for the arrays processing, pandas for preprocessing and other minor third parts libraries. Keras was configured with TensorFlow in backend to have the fastest possible training. Chosen Neural Network’s topology has a conv layer on the first layer, a dense layer (mlp) on second stage, and a dense layer on the last stage. For every training, 100 epochs were run and instead of doing earlyStopping, i prefered to save the best model within all the epochs, every time, as I noticed there could not be a precise stopping rule for this kind of data.   \n",
        "\n",
        "**Results**\n",
        "\n",
        "Because of the inner poor nature of these data attributes and the restrictions from the exercise on the usable attributes (property type, lease duration, location (london – non london) it is very difficult for any predictor forecasting the right price of next purchasing of one property in Uk. Anyway I reached good results by means of this code  \n",
        "\n",
        "**Future directions**\n",
        "\n",
        "Future improvements might be: genetic algorithm for hyper-parameter optimization, deeper networks topologies,  integration of this model with other models (grounded on computer vision, natural language processing) able to deal with other information sources: news, sentiments, etc. \n",
        "\n"
      ],
      "metadata": {
        "id": "6_sxpJ4ngKQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All Imports**"
      ],
      "metadata": {
        "id": "sjKtq0eZJqf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on 21/03/2022\n",
        "\n",
        "@author: Francesco Pugliese\n",
        "'''\n",
        "\n",
        "import warnings\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "    \n",
        "import numpy\n",
        "import os\n",
        "import sys\n",
        "import timeit\n",
        "import platform\n",
        "import argparse\n",
        "\n",
        "# Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop"
      ],
      "metadata": {
        "id": "mqY_B82CJrvH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All Initializations**"
      ],
      "metadata": {
        "id": "9GyaBYcET7ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "startTime = timeit.default_timer()\n",
        "OS = platform.system()   # Operating System\n",
        "os.system('pip install --upgrade --no-cache-dir gdown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXo57eYNT8JQ",
        "outputId": "7b605e97-86b7-4372-a369-a18c670ba653"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All Globals**"
      ],
      "metadata": {
        "id": "Yb3K7yiLaiSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_config_file = \"landregistrymodel.ini\"                                         # Default Configuration File\n",
        "defaultCallbacks = []"
      ],
      "metadata": {
        "id": "Q9sDasUqagTZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All Utility Functions Definition**"
      ],
      "metadata": {
        "id": "eufo2ByNJ2nD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9RYlwisAJYqI"
      },
      "outputs": [],
      "source": [
        "def data_download(file_to_download, gdrive_code, OS, uncompress = True):\n",
        "  if not os.path.exists(file_to_download):\n",
        "    os.system('gdown --id \"'+gdrive_code+'\" --output '+file_to_download)\n",
        "    if OS == \"Linux\" and uncompress:\n",
        "        os.system('unzip -o -n \"./'+file_to_download+'\" -d \"./\"')\n",
        "    return True\n",
        "  else: \n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dataset and all Libs download**"
      ],
      "metadata": {
        "id": "WtetM82WJ_O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = data_download(\"./pp-2020_2021.zip\", \"1NSiYUM1gK65cXS9eHGEdFaZ7dY9wU1bj\", OS)\n",
        "out = data_download(\"./uk_lr_regression_libs.zip\", \"190uP4CEAd5Tvf351S-p6iAR4mfwyxufB\", OS)"
      ],
      "metadata": {
        "id": "Weh3Bxp0NLhP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All Program Imports**"
      ],
      "metadata": {
        "id": "hgyA4AWXTdXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uk_lr_regression_libs.Models.landregistryneuralmodel import LandRegistryNeuralModel\n",
        "from uk_lr_regression_libs.Preprocessing.preprocessing import LandRegistryPreoprocessing\n",
        "from uk_lr_regression_libs.Settings.settings import SetParameters"
      ],
      "metadata": {
        "id": "qFAGwm9ITa1X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All Program Functions Definition**"
      ],
      "metadata": {
        "id": "2Trt7fn5TqFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_dim = 1, output_size = 1, optimizer='sgd', initMode = 'normal', activation = 'relu', neurons1 = 60, neurons2 = 200):\n",
        "    modelSummary = True\n",
        "    \n",
        "    # Build the Land Registry Model \n",
        "    neuralLandRegistryNeuralModel = LandRegistryNeuralModel.build(input_dim = input_dim, output_size = output_size, summary = modelSummary, neurons1 = neurons1, neurons2 = neurons2)  \n",
        "    \n",
        "    # Compile the Land Registry Model\n",
        "    neuralLandRegistryNeuralModel.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
        "    \n",
        "    return neuralLandRegistryNeuralModel"
      ],
      "metadata": {
        "id": "ryPWYPmmToUL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Program Configuration**"
      ],
      "metadata": {
        "id": "rMh9kqpJaa6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_file = default_config_file\n",
        "if config_file is None: \n",
        "    config_file = default_config_file                                                # Default Configuration File\n",
        "\n",
        "## Configuration of the File Parser\n",
        "# Read the Configuration File\n",
        "set_parameters = SetParameters(\"/content/uk_lr_regression_libs/Conf\", config_file, OS) \n",
        "\n",
        "par = set_parameters.read_config_file()\n",
        "# Set initial seed for reproducibility\n",
        "numpy.random.seed(par.seed) "
      ],
      "metadata": {
        "id": "qUq-wWnEaY17"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preprocessing**"
      ],
      "metadata": {
        "id": "3N1cH9PGaQX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Land Registry Data\n",
        "lrp = LandRegistryPreoprocessing(par.demoOnSubset)\t\t\t # Test the program on a smaller Dataset - True or Full Dataset - False\t\n",
        "datasets = lrp.loadLandRegistryData(dataPath=par.datasetPath, dataFile = par.datasetFile, fieldSeparator = par.fieldSeparator, normalizeX=par.normalizeX, normalizeY=par.normalizeY, demoOnSubset = par.demoOnSubset)\n",
        "\n",
        "trainsetX, trainsetY = datasets[0]\n",
        "testsetX, testsetY = datasets[1]\n",
        "dataXMax, dataYMax = datasets[2]\n",
        "\n",
        "trainsetX == numpy.asarray(trainsetX).astype('float32')\n",
        "testsetX == numpy.asarray(testsetX).astype('float32')\n",
        "trainsetY_tensor = tf.convert_to_tensor(trainsetY, dtype=tf.float32) \n",
        "testsetY_tensor = tf.convert_to_tensor(testsetY, dtype=tf.float32) \n",
        "trainsetX_tensor = tf.convert_to_tensor(trainsetX, dtype=tf.float32) \n",
        "testsetX_tensor = tf.convert_to_tensor(testsetX, dtype=tf.float32) \n",
        "\n",
        "# Compute the number of batches per dataset\n",
        "nTrainBatches = trainsetX.shape[0] // par.batchSize\n",
        "nTestBatches = testsetX.shape[0] // par.batchSize\n",
        "\n",
        "print ('\\n\\nBatch size: %i\\n' % par.batchSize)\n",
        "\n",
        "print ('Number of training batches: %i' % nTrainBatches)\n",
        "  \n",
        "print ('\\nTraining set values size (X): %i x %i' % (trainsetX.shape[0], trainsetX.shape[1]))\n",
        "print ('Training set target vector size (Y): %i x 1' % trainsetY.shape[0])\n",
        "print ('Sum of train set values (X): %.2f' % trainsetX.sum());\n",
        "print ('Sum of train set target (Y): %i' % trainsetY.sum());\n",
        "\n",
        "print ('Number of test batches: %i' % nTestBatches)\n",
        "print ('\\nTest set values size (X): %i x %i' % (testsetX.shape[0], testsetX.shape[1]))\n",
        "print ('Test set target vector size (Y): %i x 1' % testsetY.shape[0])\n",
        "print ('Sum of test set values (X): %.2f' % testsetX.sum());\n",
        "print ('Sum of test set target (Y): %i' % testsetY.sum());"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPYU8cL6aO8Y",
        "outputId": "74f7ee55-e0d7-499d-ace8-8c0e3322c4dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Land Registry Dara from the files...\n",
            "\n",
            "Land Ragistry Data - DataSet Folder : ./\n",
            "Land Ragistry Data - DataSet File : pp-2020_2021.csv\n",
            "\n",
            "1000000 lines read\n",
            "Number of attributes per line: 5\n",
            "\n",
            "\n",
            "Batch size: 64\n",
            "\n",
            "Number of training batches: 13121\n",
            "\n",
            "Training set values size (X): 839802 x 3\n",
            "Training set target vector size (Y): 839802 x 1\n",
            "Sum of train set values (X): 377437.50\n",
            "Sum of train set target (Y): 858\n",
            "Number of test batches: 2503\n",
            "\n",
            "Test set values size (X): 160198 x 3\n",
            "Test set target vector size (Y): 160198 x 1\n",
            "Sum of test set values (X): 65785.25\n",
            "Sum of test set target (Y): 158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainsetX[0:10])\n",
        "print(type(trainsetX))\n",
        "print(trainsetY[0:10])\n",
        "print(testsetX[0:10])\n",
        "print(testsetY[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq-J9Y-8b99w",
        "outputId": "af9dbafb-1b0b-427f-f84f-f324b9fdb7b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5 0.0 0.0]\n",
            " [0.0 0.0 0.0]\n",
            " [0.75 0.25 0.0]\n",
            " [0.75 0.25 0.0]\n",
            " [0.5 0.0 0.0]\n",
            " [0.25 0.0 0.0]\n",
            " [0.25 0.0 0.0]\n",
            " [0.75 0.0 0.0]\n",
            " [0.0 0.0 0.0]\n",
            " [0.25 0.0 0.0]]\n",
            "<class 'numpy.ndarray'>\n",
            "[0.0005270631929652084 0.0013654486864383636 0.0003877874269484953\n",
            " 0.0003877874269484953 0.0004779070402534273 0.0006690698563547982\n",
            " 0.0005516412693210989 0.0014337211207602818 0.0008192692118630182\n",
            " 0.0004287508875416462]\n",
            "[[0.25 0.0 0.0]\n",
            " [0.5 0.0 0.0]\n",
            " [0.5 0.25 0.0]\n",
            " [0.5 0.25 0.0]\n",
            " [0.0 0.0 0.0]\n",
            " [0.5 0.0 0.0]\n",
            " [0.5 0.0 0.0]\n",
            " [0.5 0.0 0.0]\n",
            " [0.5 0.0 0.0]\n",
            " [0.5 0.0 0.0]]\n",
            "[0.0005188705008465782 0.0002840133267791796 0.0004287508875416462\n",
            " 0.00032088044131301543 0.001310830738980829 0.0003412256267409471\n",
            " 0.0003195149926265771 0.00029493691627068653 0.00041236550330438584\n",
            " 0.0005460429297067017]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Algorithms \n",
        "#opt = SGD(lr=par.learningRate)                                 # Stochastic Gradient Descent Training Algorithm\n",
        "opt = RMSprop(lr=par.learningRate)                              # Stochastic Gradient Descent Training Algorithm\n",
        "\n",
        "# CallBacks definition \n",
        "checkPoint=ModelCheckpoint(os.path.join(\"./\",par.modelsPath)+\"/\"+par.modelFile, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "print ('\\n')\n",
        "\n",
        "input_dim = trainsetX.shape[1]\n",
        "neuralLandRegistryNeuralModel = create_model(input_dim, par.output_size, opt, 'normal', par.activation, par.neurons1, par.neurons2)                             # Returns the model created                        \n",
        "print ('\\n\\n')\n",
        "\n",
        "if par.saveBestModel == True:\n",
        "    defaultCallbacks = defaultCallbacks+[checkPoint]\n",
        "\n",
        "# Fit the Land Registry Model \n",
        "history = neuralLandRegistryNeuralModel.fit(trainsetX_tensor, trainsetY_tensor, validation_split = 0.3, epochs=par.epochsNumber, batch_size=par.batchSize, shuffle = False, verbose=2)\t\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wsMzbLDaC4-",
        "outputId": "d81b309e-cb3a-47a5-fc3d-be3f230b7af3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 3)                 12        \n",
            "                                                                 \n",
            " activation (Activation)     (None, 3)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 8         \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23\n",
            "Trainable params: 23\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "9186/9186 - 45s - loss: 1.9618e-05 - val_loss: 2.8893e-05 - 45s/epoch - 5ms/step\n",
            "Epoch 2/10\n",
            "9186/9186 - 35s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 35s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "9186/9186 - 38s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 38s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "9186/9186 - 35s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 35s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "9186/9186 - 34s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 34s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "9186/9186 - 34s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 34s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "9186/9186 - 37s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 37s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "9186/9186 - 37s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 37s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "9186/9186 - 37s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 37s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "9186/9186 - 34s - loss: 1.9617e-05 - val_loss: 2.8893e-05 - 34s/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('\\n\\nPredicting on %i properties of 2021...\\n' % (par.numberOfPropertiesToTest))\n",
        "\n",
        "predictions = neuralLandRegistryNeuralModel.predict(testsetX, batch_size = par.batchSize, verbose = 1)   # as the Batch Size might be greater than numberOfPropertiesToTest we test on the whole Test Set\n",
        "\n",
        "if par.saveBestModel == True:\n",
        "    bestNeuralLandRegistryNeuralModel = LandRegistryNeuralModel.build(input_dim = input_dim, output_size = par.output_size, summary = False, neurons1 = par.neurons1, neurons2 = par.neurons2)  \n",
        "    bestNeuralLandRegistryNeuralModel.load_weights(os.path.join(\"./\",par.modelsPath)+\"/\"+par.modelFile)       \n",
        "    bestNeuralLandRegistryNeuralModel.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
        "    bestPredictions = bestNeuralLandRegistryNeuralModel.predict(testsetX, batch_size = par.batchSize, verbose = 1)   # as the Batch Size might be greater than numberOfPropertiesToTest we test on the whole Test Set\n",
        "\n",
        "if par.printTestModel == True: \n",
        "  logfile = open(os.path.join(\"./\",par.logPath)+\"/\"+par.logFile, \"w\")\n",
        "\n",
        "  numpy.set_printoptions(precision = par.logPrecision, suppress = True)\n",
        "  print (\"\\n\\n\\nLast Model: Real Price and Predicted Price : \\n\") \n",
        "  print (\"\\n\\n\\nLast Model: Real Price and Predicted Price : \\n\", file = logfile) \n",
        "  \n",
        "  # Denormalize outputs\n",
        "  if par.normalizeY == True:\n",
        "      testsetY = testsetY * dataYMax\n",
        "      predictions = predictions * dataYMax\n",
        "      if par.saveBestModel == True:\n",
        "          bestPredictions = bestPredictions * dataYMax\n",
        "  \n",
        "  for i in range(par.numberOfPropertiesToTest):\n",
        "      print (\"%i: RT %i - PT %i\" % (i, testsetY[i,], numpy.round(predictions[i,], par.logPrecision)))\n",
        "      print (\"%i: RT %i - PT %i\" % (i, testsetY[i,], numpy.round(predictions[i,], par.logPrecision)), file = logfile)\n",
        "\n",
        "  if par.saveBestModel == True:\n",
        "      print (\"\\n\\n\\nBest Model: Real Price and Predicted Price : \\n\") \n",
        "      print (\"\\n\\n\\nBest Model: Real Price and Predicted Price : \\n\", file = logfile) \n",
        "      for i in range(par.numberOfPropertiesToTest):\n",
        "          print (\"%i: RT %i - PT %i\" % (i, testsetY[i,], numpy.round(bestPredictions[i,], par.logPrecision)))\n",
        "          print (\"%i: RT %i - PT %i\" % (i, testsetY[i,], numpy.round(bestPredictions[i,], par.logPrecision)), file = logfile)\n",
        "\n",
        "  logfile.close()\n",
        "\n",
        "endTime = timeit.default_timer()\n",
        "print ('\\nTotal time: %.2f minutes' % ((endTime - startTime) / 60.))"
      ],
      "metadata": {
        "id": "C05fdGSzBfKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IZ29LPw6J-UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homeworks**"
      ],
      "metadata": {
        "id": "TMLLOHbye6Rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0) Make this code run\n",
        "\n",
        "1) Write the same regression code for the dataset Boston Housing Price regression of keras: \n",
        "https://keras.io/api/datasets/boston_housing/\n",
        "\n",
        "Hint: rewrite the preprocessing function\n",
        "\n",
        "2) Write a more complex model with Conv1D as feature extractors\n",
        "\n",
        "3) Plot the history and predictions"
      ],
      "metadata": {
        "id": "Jr5BVH8ze-0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6BmYp3oWe9_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}