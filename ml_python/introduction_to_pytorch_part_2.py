# -*- coding: utf-8 -*-
"""introduction_to_pytorch_part_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iV1Qmbur7QpgNlyLOay4pjAfj2W06b-5

#**Introduction to PyTorch Part 2**

#**All General Imports**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# %matplotlib inline
from matplotlib import pyplot as plt

"""#**All PyTorch Imports**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

"""#**Softmax**

We need our network output a single scalar value (so n_out = 1), cast the labels to floats (0.0 for airplane and 1.0 for bird), and use those as a target for MSELoss (the average of squared differences in the batch). Doing so, we would cast the problem into a regression problem. However, looking more closely, we are now dealing with something a bit different in nature. We need to recognize that the output is categorical: it’s either a bird or an airplane (or something else if we had all 10 of the original classes in case of Cifa10). We have to represent a categorical variable, we should switch to a one-hot-encoding representation of that variable, such as [1, 0] for airplane or [0, 1] for bird (the order is arbitrary). This will still work if we have 10 classes, as in the full CIFAR-10 dataset; we’ll just have a vector of length 10.In the ideal case, the network would output torch.tensor([1.0, 0.0]) for an airplane and torch.tensor([0.0, 1.0]) for a bird. Practically speaking, since our classifier will not be perfect, we can expect the network to output something in between. The key realization in this case is that we can interpret our output as probabilities: the first entry is the probability of “airplane,” and the second is the probability of “bird.” Converting the problem in terms of probabilities imposes a few extra constraints on the outputs of our network: Each element of the output must be in the [0.0, 1.0] range (a probability of an outcome cannot be less than 0 or greater than 1). The elements of the output must add up to 1.0 (we’re certain that one of the two outcomes will occur). It sounds like a tough constraint to enforce in a differentiable way on a vector of numbers. Yet there’s a very smart trick that does exactly that, and it’s differentiable: it’s called softmax. Representing the output as probabilities, Softmax is a function that takes a vector of values and produces another vector of the same dimension, where the values satisfy the constraints we just listed to represent probabilities. That is, we take the elements of the vector, compute the elementwise exponential, and divide each element by the sum of exponentials:

![pasted image 0.png](data:image/png;base64,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)
"""

def softmax(x):
    return torch.exp(x) / torch.exp(x).sum()

x = torch.tensor([1.0, 2.0, 3.0])

print(softmax(x))
print(softmax(x).sum())

"""Softmax is a monotone function, in that lower values in the input will correspond to lower values in the output. However, it’s not scale invariant, in that the ratio between values is not preserved. In fact, the ratio between the first and second elements of the input is 0.5, while the ratio between the same elements in the output is 0.3678. This is not a real issue, since the learning process will drive the parameters of the model in a way that values have appropriate ratios.

The nn module of PyTorch makes softmax available as a module. Since, as usual, input tensors may have an additional batch 0th dimension, or have dimensions along which they encode probabilities and others in which they don’t, **nn.Softmax** requires us to specify the dimension along which the softmax function is applied.

**nn.Softmax:** applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.
"""

softmax = nn.Softmax(dim=1)

x = torch.tensor([[1.0, 2.0, 3.0],
                  [1.0, 2.0, 3.0]])

softmax(x)

"""#**Loss for Classification**

In the past, we mentioned that the loss is what gives probabilities meaning. For instance, we used the **Mean Square Error (MSE)**. We could still use MSE and make our output probabilities converge to [0.0, 1.0] and [1.0, 0.0]. However, thinking about it, we’re not really interested in reproducing these values exactly. Looking back at the argmax operation we used it (in keras) to extract the index of the predicted class, what we’re really interested in is that the first probability is higher than the second for airplanes and vice versa for birds, for example. In other words, we want to penalize misclassifications rather than penalize everything that doesn’t look exactly like a 0.0 or 1.0. What we need to maximize in this case is the probability associated with the correct class, out[class_index], where out is the output of softmax and class_index is a vector containing 0 for “airplane” and 1 for “bird” for each sample. This quantity--that is, the probability associated with the correct class--is referred to as the likelihood (of our model’s parameters, given the data). In other words, we want a loss function that is very high when the likelihood is low: so low that the alternatives have a higher probability. Conversely, the loss should be low when the likelihood is higher than the alternatives, and we’re not really fixated on driving the probability up to 1. There’s a loss function that behaves that way, and it’s called negative log likelihood (NLL). It has the expression NLL = - sum(log(out_i[c_i])), where the sum is taken over N samples and c_i is the correct class for sample i. When low probabilities are assigned to the data, the NLL grows to infinity, whereas it decreases at a rather shallow rate when probabilities are greater than 0.5. Remember that the NLL takes probabilities as input; so, as the likelihood
likelihood grows, the other probabilities will necessarily decrease.

Summing up, our loss for classification can be computed as follows. For each sample in the batch: Run the forward pass, and obtain the output values from the last (linear) layer. Compute their softmax, and obtain probabilities. Take the predicted probability corresponding to the correct class (the likelihood of the parameters). Note that we know what the correct class is because it’s a supervised problem--it’s our ground truth. Compute its logarithm, slap a minus sign in front of it, and add it to the loss.

So, how do we do this in PyTorch? PyTorch has an nn.NLLLoss class. However, as opposed to what you might expect, it does not take probabilities but rather takes a tensor of log probabilities as input. It then computes the NLL of our model given the batch of data. There’s a good reason behind the input convention: taking the logarithm of a probability is tricky when the probability gets close to zero. The workaround is to use nn.LogSoftmax instead of nn.Softmax, which takes care to make the calculation numerically stable.
"""

model = nn.Sequential(
             nn.Linear(3072, 512),
             nn.Tanh(),
             nn.Linear(512, 2),
             nn.LogSoftmax(dim=1))

loss = nn.NLLLoss()

"""The loss takes the output of nn.LogSoftmax for a batch as the first argument and a tensor of class indices (zeros and ones, in our case) as the second argument.

"""

img, label = cifar2[0]
out = model(img.view(-1).unsqueeze(0))
loss(out, torch.tensor([label]))

"""Ending our investigation of losses, we can look at how using cross-entropy loss improves over MSE. We see that the cross-entropy loss has some slope when the prediction is off target (in the low-loss corner, the correct class is assigned a predicted probability of 99.97%), while the MSE we dismissed at the beginning saturates much earlier and--crucially--also for very wrong predictions. The underlying reason is that the slope of the MSE is too low to compensate for the flatness of the softmax function for wrong predictions. This is why the MSE for probabilities is not a good fit for classification work.

#**Pytorch nn module**

**PyTorch** has a whole submodule dedicated to neural networks, called torch.nn. It contains the building blocks needed to create all sorts of neural network architectures. Those building blocks are called modules in PyTorch (such building blocks are often referred to as layers in other frameworks). A PyTorch module is a Python class deriving from the nn.Module base class. A module can have one or more Parameter instances as attributes, which are tensors whose values are optimized during the training process (think w and b in our linear model). A module can also have one or more submodules (subclasses of nn.Module) as attributes, and it will be able to track their parameters as well. Unsurprisingly, we can find a subclass of nn.Module called **nn**.**Linear**, which applies an affine transformation to its input (via the parameter attributes weight and bias) and is equivalent to Dense layers without activation layer in Keras.

All PyTorch-provided subclasses of **nn.Module** have their __call__ method defined. This allows us to instantiate an nn.Linear and call it as if it was a function.
"""

t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c).unsqueeze(1)
t_u = torch.tensor(t_u).unsqueeze(0)

t_u.shape
t_u

t_u

"""#**Shuffling of Tensors**"""

n_samples = t_u.shape[0]
n_val = int(0.2 * n_samples)

shuffled_indices = torch.randperm(n_samples)

train_indices = shuffled_indices[:-n_val]
val_indices = shuffled_indices[-n_val:]

train_indices, val_indices

t_u_train = t_u[train_indices]
t_c_train = t_c[train_indices]

t_u_val = t_u[val_indices]
t_c_val = t_c[val_indices]

t_un_train = 0.1 * t_u_train
t_un_val = 0.1 * t_u_val

"""**nn.Linear:** applies a linear transformation to the incoming data: y = xA^T + b. https://pytorch.org/docs/stable/generated/torch.nn.Linear.html

The constructor to nn.Linear accepts three arguments: the number of input features, the number of output features, and whether the linear model includes a bias or not (defaulting to True, here).
"""

import torch.nn as nn
t_u_val = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
linear_model = nn.Linear(1, 1)
linear_model(t_un_val)

"""Calling an instance of nn.Module with a set of arguments ends up calling a method named forward with the same arguments. The forward method is what executes the forward computation, while __call__ does other rather important chores before and after calling forward. So, it is technically possible to call forward directly, and it will produce the same output as __call__, but this should not be done from user code:"""

y = model(x)       
y = model.forward(x)

"""#**Initializations**"""

torch.set_printoptions(edgeitems=2)
torch.manual_seed(123)

"""#**Globals**"""

data_path = './'
model_path = './'
class_names = ['airplane','automobile','bird','cat','deer', 'dog','frog','horse','ship','truck']

"""#**Hyper-parameters**"""

n_epochs = 4
learning_rate = 1e-2
n_out = 2

"""#**Datasets Downloads**"""

cifar10 = datasets.CIFAR10(
    data_path, train=True, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4915, 0.4823, 0.4468),
                             (0.2470, 0.2435, 0.2616))
    ]))

cifar10_val = datasets.CIFAR10(
    data_path, train=False, download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4915, 0.4823, 0.4468),
                             (0.2470, 0.2435, 0.2616))
    ]))

len(cifar10)

len(cifar10_val)

"""#**Data Preparation**

We are willing to discriminate between **airplanes** and **birds** in order to detect them during the flight, for example.

So we create subclasses that only includes birds and airplanes. So we take a shortcut and just filter the data in cifar10 and remap the labels so they are contiguous.
"""

label_map = {0: 0, 2: 1}
class_names = ['airplane', 'bird']

"""We then create a **cifar2** object which satisfies the basic requirements for a Dataset--that is, __len__ and __getitem__ are defined--so we’re going to use that. We should be aware, however, that this is a clever shortcut and we might wish to implement a proper Dataset if we hit limitations with it. """

cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]
cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]

len(cifar2)

len(cifar2_val)

"""#**Fully Connected Layers**

#**Model Definition**

Number of Features:**32 × 32 × 3** which is **3,072** input features per sample. Our model is an nn.Linear with 3,072 input features and some number of hidden features, followed by an activation, and then another nn.Linear that tapers the network down to an appropriate output number of features (2, for this use case). We create 512 hidden features. A neural network like an MLP (Multi Layer Perceptron) needs at least one hidden layer (of activations, so two modules) with a nonlinearity in between in order to be able to learn arbitrary functions, it would just be a linear model.

**nn.Sequential:** represents a **Sequential** container. The value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores. https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html

**nn.Tanh:** applies the Hyperbolic Tangent (Tanh) function element-wise. https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html

The **hidden features** represent "learned" relations between the inputs encoded through the weight matrix. As such, the model might learn to “compare” vector elements 176 and 208, but it does not a priori focus on them because it is structurally unaware that these are, indeed (row 5, pixel 16) and (row 6, pixel 16), and thus adjacent. So we have a model.
"""

model = nn.Sequential(
            nn.Linear(3072, 512),
            nn.Sigmoid(),
            nn.Linear(512, 2),
            nn.Softmax(dim=1))

"""Let’s do it, just to see what comes out. We first build a batch of one image, our bird"""

img, _ = cifar2[0]

plt.imshow(img.permute(1, 2, 0))
plt.show()

"""In order to call the model, we need to make the input have the right dimensions. We recall that our model expects 3,072 features in the input, and that nn works with data organized into batches along the zeroth dimension. So we need to turn our 3 × 32 × 32 image into a 1D tensor and then add an extra dimension in the zeroth position. """

img_batch = img.view(-1).unsqueeze(0)

"""Now we can call our model. So, we get probabilities. At moment, the weights and biases of our linear layers have not been trained at all. Their elements are initialized randomly by PyTorch between -1.0 and 1.0. Interestingly, we also see grad_fn for the output, which is the tip of the backward computation graph In addition, while we know which output probability is supposed to be which (recall our class_names), our network has no indication of that. Is the first entry “airplane” and the second “bird,” or the other way around? The network can’t even tell that at this point. It’s the loss function that associates a meaning with these two numbers, after backpropagation. """

out = model(img_batch)
out

"""Thus, after training, we will be able to get the label as an index by computing the argmax of theoutput probabilities: that is, the index at which we get the maximum probability. Conveniently, when supplied with a dimension, torch.max returns the maximum element along that dimension as well as the index at which that value occurs. In our case, we need to take the max along the probability vector (not across batches), therefore, dimension 1. """

_, index = torch.max(out, dim=1)

index

"""#**Model Training**

We apply changes to parameters based on a very partial estimation of the gradient on a single sample. However, what is a good direction for reducing the loss based on one sample might not be a good direction for others. By shuffling samples at each epoch and estimating the gradient on one or (preferably, for stability) a few samples at a time, we are effectively introducing randomness in our gradient descent. SGD stands for stochastic gradient descent, and this is what the S is about: working on small batches (aka minibatches) of shuffled data. It turns out that following gradients estimated over minibatches, which are poorer approximations of gradients estimated across the whole dataset, helps convergence and prevents the optimization process from getting stuck in local minima it encounters along the way. Gradients from minibatches are randomly off the ideal trajectory, which is part of the reason why we want to use a reasonably small learning rate. Shuffling the dataset at each epoch helps ensure that the sequence of gradients estimated over minibatches is representative of the gradients computed across the full dataset. Typically, minibatches are a constant size that we need to set prior to training, just like the learning rate. These are the hyperparameters, to distinguish them from the parameters of a model.

**nn.NLLLoss**:The negative log likelihood loss. It is useful to train a classification problem with C classes. Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer. https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html
"""

model = nn.Sequential(
            nn.Linear(3072, 512),
            nn.Tanh(),
            nn.Linear(512, 2),
            nn.LogSoftmax(dim=1))

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

loss_fn = nn.NLLLoss()

for epoch in range(n_epochs):
    for img, label in cifar2:
        out = model(img.view(-1).unsqueeze(0))
        loss = loss_fn(out, torch.tensor([label]))
                
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print("Epoch: %d, Loss: %f" % (epoch, float(loss)))

"""In our previous training code, we chose minibatches of size 1 by picking one item at a time from the dataset. The torch.utils.data module has a class that helps with shuffling and organizing the data in minibatches: **DataLoader**. The job of a data loader is to sample minibatches from a dataset, giving us the flexibility to choose from different sampling strategies. A very common strategy is uniform sampling after shuffling the data at each epoch.

At each inner iteration, imgs is a tensor of size 64 × 3 × 32 × 32--that is, a minibatch of 64 (32 × 32) RGB images--while labels is a tensor of size 64 containing label indices.

At a minimum, the **DataLoader** constructor takes a Dataset object as input, along with batch_size and a shuffle Boolean that indicates whether the data needs to be shuffled at the beginning of each epoch:
"""

train_loader = DataLoader(cifar2, batch_size=64, shuffle=True)

"""The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss. This terminology is a particularity of PyTorch, as the nn.NLLoss computes, in fact, the cross entropy but with log probability predictions as inputs where nn.CrossEntropyLoss takes scores (sometimes called logits). Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs."""

model = nn.Sequential(
            nn.Linear(3072, 128),           # after training check with 512
            nn.Tanh(),
            nn.Linear(128, 2),
            nn.LogSoftmax(dim=1))

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

loss_fn = nn.NLLLoss()

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        loss = loss_fn(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print("Epoch: %d, Loss: %f" % (epoch, float(loss)))

"""We can compute the accuracy of our model on the training set and validation set in terms of the number of correct classifications over the total.

**torch.no_grad():** Context-manager that disabled gradient calculation.Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward()
"""

train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
                                           shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in train_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Accuracy: %f" % (correct / total))

val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,
                                         shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in val_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Accuracy: %f" % (correct / total))

"""This cross entropy can be interpreted as a negative log likelihood of the predicted distribution under the target distribution as an outcome. So both losses are the negative log likelihood of the model parameters given the data when our model predicts the (softmax-applied) probabilities."""

train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)

model = nn.Sequential(
            nn.Linear(3072, 1024),
            nn.Tanh(),
            nn.Linear(1024, 512),
            nn.Tanh(),
            nn.Linear(512, 128),
            nn.Tanh(),
            nn.Linear(128, 2))

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

loss_fn = nn.CrossEntropyLoss()

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        loss = loss_fn(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print("Epoch: %d, Loss: %f" % (epoch, float(loss)))

train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
                                           shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in train_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Accuracy: %f" % (correct / total))

val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,
                                         shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in val_loader:
        outputs = model(imgs.view(imgs.shape[0], -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Accuracy: %f" % (correct / total))

"""PyTorch offers a quick way to determine how many parameters a model has through the parameters() method of nn.Model (the same method we use to provide the parameters to the optimizer). To find out how many elements are in each tensor instance, we can call the numel method. Summing those gives us our total count. Depending on our use case, counting parameters might require us to check whether a parameter has requires_grad set to True, as well. We might want to differentiate the number of trainable parameters from the overall model size."""

sum([p.numel() for p in model.parameters()])

sum([p.numel() for p in nn.Linear(3072, 512).parameters()])

"""#**Convolutional Layers**

The torch.nn module provides convolutions for 1, 2, and 3 dimensions: **nn.Conv1d** for time series, **nn.Conv2d** for images, and **nn.Conv3d** for volumes or videos.

At a minimum, the arguments we provide to nn.Conv2d are the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel), the number of output features, and the size of the kernel. For instance, for our first convolutional module, we’ll have 3 input features per pixel (the RGB channels) and an arbitrary number of channels in the output--say, 16. The more channels in the output image, the more the capacity of the network. We need the channels to be able to detect many different types of features. Also, because we are randomly initializing them, some of the features we’ll get, even after training, will turn out to be useless.2 Let’s stick to a kernel size of 3 × 3.It is very common to have kernel sizes that are the same in all directions, so PyTorch has a shortcut for this: whenever kernel_size=3 is specified for a 2D convolution, it means 3 × 3 (provided as a tuple (3, 3) in Python).
"""

train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(8 * 8 * 8, 32)
        self.fc2 = nn.Linear(32, 2)
        
    def forward(self, x):
        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)
        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)
        out = out.view(-1, 8 * 8 * 8)
        out = torch.tanh(self.fc1(out))
        out = self.fc2(out)
        return out
    
model = Net()

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

loss_fn = nn.CrossEntropyLoss()

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        outputs = model(imgs)
        loss = loss_fn(outputs, labels)
                
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print("Epoch: %d, Loss: %f" % (epoch, float(loss)))

train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
                                           shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in train_loader:
        outputs = model(imgs)
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Accuracy: %f" % (correct / total))

val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,
                                         shuffle=False)

correct = 0
total = 0

with torch.no_grad():
    for imgs, labels in val_loader:
        outputs = model(imgs)
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())
        
print("Accuracy: %f" % (correct / total))

model = Net()
sum([p.numel() for p in model.parameters()])

"""#**Display Validation Accuracy for each epoch**"""

'''
model = Net()

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

loss_fn = nn.CrossEntropyLoss()

for epoch in range(n_epochs):
    for (imgs, labels), (val_imgs, val_labels) in zip(train_loader, val_loader):
        outputs = model(imgs)
        val_outputs = model(val_imgs)
        loss = loss_fn(outputs, labels)
                
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    #Accuracy
    val_outputs = (val_outputs > 0.5).float()
    correct = (val_outputs == val_labels).float().sum()
    print("Epoch: %d, Loss: %f Accuracy: %f" % (epoch, float(loss), correct/val_cifar2.shape[0]))
'''

"""#**Mnist Training**"""

train_loader = torch.utils.data.DataLoader(
    datasets.MNIST(data_path, train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor()#,
                       #transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=64, shuffle=True)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

model = Net()

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

for epoch in range(10):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
    print('Current loss', float(loss))

torch.save(model.state_dict(), model_path+'mnist.pth')

pretrained_model = Net()
pretrained_model.load_state_dict(torch.load(model_path+'mnist.pth'))