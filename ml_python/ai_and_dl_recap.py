# -*- coding: utf-8 -*-
"""ai_and_dl_recap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GUknkPLsno6YE6DR1WG2yDdiHOtaM9MI

# **Artificial Intelligence and Deep Learning: Recap**

# Artificial Neural Networks 

Artificial Neuron vs Biological Neuron

<img src="https://miro.medium.com/max/1220/1*SJPacPhP4KDEB1AdhOFy_Q.png" alt="drawing" width="500"/>

## **Weights** **$w_i$**

$ potential = bias + \Sigma_{i=1}^n w_i \cdot in_i$

## **Activation $f(\cdot)$**

$out = f(potential)$

### **Sigmoid:**
$f(x) = \frac{1}{1 + e^{-x}} \in ]0, 1[$
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Activation

from matplotlib import pyplot as plt

activation_layer = Activation('sigmoid')
x = tf.linspace(-30.0, 30.0, 100)
y = activation_layer(x)

fig, ax = plt.subplots(figsize=(10, 6))
plt.plot(x, y)
plt.grid()
plt.show()

"""### **Hypoerbolic tangent**
$f(x) = \tanh(x) \in ]-1, 1[$
"""

activation_layer = Activation('tanh')
x = tf.linspace(-30.0, 30.0, 100)
y = activation_layer(x)

fig, ax = plt.subplots(figsize=(10, 6))
plt.plot(x, y)
plt.grid()
plt.show()

"""### **Rectified Linear Unit (RELU)**
$f(x) = max(0, x) \in [0, \rightarrow$
"""

activation_layer = Activation('relu')
x = tf.linspace(-3.0, 3.0, 100)
y = activation_layer(x)

fig, ax = plt.subplots(figsize=(10, 6))
plt.plot(x, y)
plt.grid()
plt.show()

"""# **Artificial Neural Networks (Multilayer Perceptrons)**

### **Layers**

- Neurons are made of layers

### **Network**

- Layers are stacked into a neural network

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/1280px-Artificial_neural_network.svg.png" alt="drawing" width="500"/>

## **Training**

### **Data Preparation**

- Inputs must be numerical and in a pretty defined range

  - Numbers should be normalized or standardized

  - Pixel / Sound intensity are numbers

  - Categorical data can be One-Hot encoded

  - Texts (vocabulary) can be indexed (e.g. bag of words) or word embeddings are used

### **Stochastic Gradient Descent**

- Forward Pass

<img src="https://thumbs.gfycat.com/ContentDarlingCub.webp" alt="drawing" width="800"/>

- Backpropagation Pass

<img src="https://thumbs.gfycat.com/HelpfulConstantKite-small.gif" alt="drawing" width="800"/>

### **Weight Updates**

To make the learning process less chaotic and more stable, one may use:

- Batch learning

- Momentum

- Learning Rate Decay

## **Prediction**

- To make a prediction, you just need the network topology and all the weights.

<img src="https://thumbs.gfycat.com/AdorableJoyfulLemming.webp" alt="drawing" width="800"/>

# **Deep Learning Frameworks**

[Which deep learning framework is the best?](https://towardsdatascience.com/which-deep-learning-framework-is-the-best-eb51431c39a)

## **Keras**
[Deep Learning for Humans](https://keras.io/)

## **Tensorflow**

[An end-to-end open source machine learning platform](https://www.tensorflow.org/)

## **Pytorch**
[From research to production](https://pytorch.org/)

# **Layers**

Neural networks are organized in layers

## **Perceptron**
- The **perceptron** is the smallest neural network, with a single neuron.
- It can do what a _Linear regression_ or a _Logistic Regression_ can do

![classifier](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/1000px-Perceptron_example.svg.png)

## **Dense Layers**

Also called **Fully connected layers**

- Fully connected layers are the normal at feedforward neural network layer.

- These layers may have a nonlinear activation function or a softmax activation in order to output probabilities of class predictions. 

- Fully connected layers are used at the end of the network after feature extraction and consolidation has been performed by the convolutional and pooling layers.

- They are used to create final nonlinear combinations of features and for making predictions by the network.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <img src="https://www.researchgate.net/profile/Martin-Cenek/publication/325067027/figure/fig5/AS:669059146342414@1536527539867/Illustration-of-the-multilayer-perceptron-artificial-neural-network-implementation-for.png"/>

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([Dense(units=4, input_shape=[6], activation='relu'), layers.Dense(units=3, activation='relu'),  layers.Dense(units=1, activation=None)])

model.summary()

"""## Convolutional Layers

- **Convolutional Neural Networks** are a powerful artificial neural network technique.

- These networks preserve the spatial structure of the problem and were developed for object recognition tasks, such as handwritten digit recognition. 

- They are popular because people are achieving state-of-the-art results on difficult computer vision and natural language processing tasks.

- **Convolutional layers** and **Pooling layers** and **Dense layers** are the buidling blocks of CNNs.

- Advantages of CNNs over MLPs in the case of images:

  - They use fewer parameters (weights) to learn than a fully connected
  network.

  - They are designed to be invariant to object position and distortion in
  the scene.

  - They automatically learn and generalize features from the input domain.

<img src="https://i0.wp.com/developersbreach.com/wp-content/uploads/2020/08/cnn_banner.png" 
alt="drawing" width="1000"/>

### Filters

- The filters are essentially the neurons of the layer. 

- They have both weighted inputs and generate an output value like a neuron.

- The input size is a fixed square called a _receptive field_.
  
  - If the convolutional layer is an input layer, then the input patch will be pixel values.
  
  - If they're deeper in the network architecture, then the convolutional layer will take input from a _feature map_ from the previous layer.

<img src="https://www.researchgate.net/publication/316950618/figure/fig4/AS:495826810007552@1495225731123/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.png" 
alt="drawing" width="300"/>

### Feature Maps

- The _feature map_ is the output of one filter applied to the previous layer.

- A given _filter_ is drawn across the entire previous layer and moved one pixel at a time. Each position results in an activation of the neuron and the output is collected in the _feature map_. 

- **Stride**: If the receptive field is moved one pixel from activation to activation, then the field will overlap with the previous activation. The distance that filter is moved across the input from the previous layer each activation is referred to as the stride. 

- **Padding**: If the size of the previous layer is not cleanly divisible by the size of the filter's receptive field and the size of the stride then it is possible for the receptive field to attempt to read off the edge of the input feature map. In this case, techniques like _zero padding_ can be used to invent mock inputs with zero values for the receptive field to read.

<img src="https://miro.medium.com/max/790/1*1okwhewf5KCtIPaFib4XaA.gif" 
alt="drawing" width="300"/>

## Pooling Layers

- The pooling layers **down-sample** the previous layers feature map.

- Pooling layers follow a sequence of one or more convolutional layers and are intended to **consolidate the features learned** and
expressed in the previous layer's feature map.

- As such, pooling may be considered a technique to **compress or generalize feature representations** and generally reduce the overfitting of the training data by the model.

- Receptive field & Stride:

  Their receptive field is much smaller than the convolutional layer.
  
  The stride is often equal to the size of the receptive field to avoid any overlap. 
  
- Pooling layers are often very simple, taking
the _average_ or the _maximum_ of the input value in order to create its own feature map.

<img src="https://developers.google.com/machine-learning/practica/image-classification/images/maxpool_animation.gif"
alt="drawing" width="500"/>

## Best practices

- **Input Receptive Field Dimensions**: The default is 2D for images, but could be 1D such as for words in a sentence, signals in a time series or 3D for video that adds a time dimension.

- **Receptive Field Size**: The patch should be as small as possible, but large enough to see features in the input data. It is common to use 3 x 3 on small images and 5 x 5 or 7 x 7 and more on larger image sizes.

- **Stride Width**: Use the default stride of 1. It is easy to understand and you don't need padding to handle the receptive field falling off the edge of your images. This could be increased to 2 or larger for larger images.

- **Number of Filters**: Filters are the feature detectors. Generally fewer filters are used at the input layer and increasingly more filters used at deeper layers.

- **Padding**: Set to zero and called zero padding when reading non-input data. This is useful when you cannot or do not want to standardize input image sizes or when you want to use receptive field and stride sizes that do not neatly divide up the input image size.

- **Pooling**: Pooling is a destructive or generalization process to reduce overfitting. Receptive field size is almost always set to 2 x 2 with a stride of 2 to discard 75% of the activations from the output of the previous layer.

- **Data Preparation**: Consider standardizing input data, both the dimensions of the images and pixel values.

- **Pattern Architecture**: It is common to pattern the layers in your network architecture. This might be one, two or some number of convolutional layers followed by a pooling layer. This structure can then be repeated one or more times. Finally, fully connected layers are
often only used at the output end and may be stacked one, two or more deep.

- **Dropout**: CNNs have a habit of overfitting, even with pooling layers. Dropout should be used such as between fully connected layers and perhaps after pooling layers.

# Stochastic Gradient Descent
- Sample some training data, run it through the network to make predictions.

- Measure the loss between the predictions and the true values.

- Adjust the weights in a direction that makes the loss smaller.

- Repeat until the loss is small enough / does not decrease anymore.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <img src="https://i.imgur.com/rFI1tIk.gif"/ width="100%">

"""## **Under- & overfitting**

- A model with too little capacity underfits
- A model with too much capacity might overfit

![fitting](https://i.imgur.com/eUF6mfo.png)

## **Learning curves**

The learning curves help the ML engineer, data scientist, to see when the model over- or underfits the training data.

<img src="https://i.imgur.com/eP0gppr.png" alt="drawing" width="700"/>

# **State of the Art Architectures**

## **For Images**

these models were trained on heaps of images and won the ImageNet competition.

<img src="https://machinelearningmastery.com/wp-content/uploads/2017/08/Sample-of-Images-from-the-ImageNet-Dataset-used-in-the-ILSVRC-Challenge.png"/>

### Oxford's [VGG](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)

<img src="https://vitalflux.com/wp-content/uploads/2021/11/VGG16-CNN-Architecture.png" alt="drawing" width="800"/>
"""

from keras.applications.vgg16 import VGG16
model = VGG16()
model.summary()

"""### Google's [Inception](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)

<img src="https://miro.medium.com/max/1024/1*cwR_ezx0jliDvVUV6yno5g.jpeg"/>

<img src="https://production-media.paperswithcode.com/methods/inceptionv3onc--oview_vjAbOfw.png" alt="drawing" width="1000"/>
"""

from keras.applications.inception_v3 import InceptionV3
model = InceptionV3()
model.summary()

"""### Microsoft's [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network)

<img src="https://www.researchgate.net/publication/331364877/figure/fig3/AS:741856270901252@1553883726825/Left-ResNet50-architecture-Blocks-with-dotted-line-represents-modules-that-might-be.png" alt="drawing" width="800"/>

"""

from keras.applications.resnet import ResNet50
model = ResNet50()
model.summary()

"""## **For Text**

- feature-extraction (get the vector representation of a text)
- fill-mask
- ner (named entity recognition)
- question-answering
- sentiment-analysis
- summarization
- text-generation
- translation
- zero-shot-classification

### **Transformers**

- were introduced in 2017: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
- took NLP by storm
- might revolutionize the images sector as well [An image is worth 16 x 16 words](https://arxiv.org/pdf/2010.11929.pdf), 2020
- [The illustrated transformer](https://jalammar.github.io/illustrated-transformer/)

### Short history

- GPT-like (also called auto-regressive Transformer models, generate text)
- BERT-like (also called auto-encoding Transformer models, ingest text)
- BART/T5-like (also called sequence-to-sequence Transformer models)

![transformers](https://huggingface.co/course/static/chapter1/transformers_chrono.png)

### **Transfer learning**

- More data, more parameters, that's the way to better transformers

<img src="https://huggingface.co/course/static/chapter1/model_parameters.png" alt="drawing" width="800"/>

- Training such state-of-the-art models is expensive in time, resources, data, environmental impact

![co2](https://huggingface.co/course/static/chapter1/carbon_footprint.png)

- Transfer learning is like recycling existing models by fine-tuning them on your exact task.

![recycling](https://www.chemietechnik.de/assets/images/3/zwischenablage01-296bf6a5-5f76a2f3.jpg)

- [Big Science](https://bigscience.huggingface.co/): A one-year long research workshop on large multilingual models and datasets
"""