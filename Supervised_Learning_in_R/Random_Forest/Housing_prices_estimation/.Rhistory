var(rf.boston$rsq)
predicted=rf.boston$predicted
y=boston$medv
1 - sum((y-predicted)^2)/sum((y-mean(y))^2)
y
predicted=rf.boston$predicted
y=boston$medv[train]
1 - sum((y-predicted)^2)/sum((y-mean(y))^2)
rf.boston
plot(rf.boston)
#vamos a intentar para cada varlo de mtry posible
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
rf.boston
rf.boston$oob.times
300*0.6
#vamos a intentar para cada varlo de mtry posible
oob.err = double(13)
test.err = double(13)
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
fit$mse
fit$mse
plot(fit$mse)
#vamos a intentar para cada valor de mtry posible
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350] #por que elijo aqui 350?
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
ap<-available.packages()
ap
head(ap)
ap<-data.frame(available.packages())
head(ap)
"tree" %in% ap$Package
install.packages("tree")
version
library("MASS")
data(package="MASS")
library("MASS")
data(package="MASS")
boston<-Boston
dim(boston)
sapply(boston,class)
head(boston)
boston[sample(1:nrow(boston),3)]
boston[sample(1:nrow(boston),3),]
boston[sample(1:nrow(boston),10),]
boston[sample(1:nrow(boston)),]
sample(1:nrow(boston))
sample(1:nrow(boston))
sample(1:nrow(boston))
sample(1:nrow(boston))
sample(1:nrow(boston))
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
boston<-boston[sample(1:nrow(boston)),]
dim(boston)
train = sample(1:nrow(boston), boston)
train = sample(1:nrow(boston), 300)
head(boston)
train
my_data_train<-boston[train,]
?randomForest
?randomForest
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
dim(boston)
14/3
13/3
plot(rf.boston)
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=100)
rf.boston
plot(rf.boston)
par(mfrow=c(2,1))
narboles=500
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles))
narboles=500
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles))
narboles=500
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles))
par(mfrow=c(2,1))
narboles=500
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles))
narboles=500
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles))
par(mfrow=c(2,1))
narboles=500
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles))
narboles=100
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles))
narboles=500
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles),xlim=c(0,500))
narboles=100
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=narboles)
rf.boston
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston,main=paste("Error vs. ntrees, ntree=",narboles),xlim=c(0,500))
oob.err = double(ncol(boston)-1)
test.err = double(ncol(boston)-1)
nvariables<-5
fit = randomForest(medv~., data = boston, subset=train, mtry=nvariables, ntree = 350)
fit
oob.err[nvariables] = fit$mse[350]
oob.err
pred = predict(fit, boston[-train,])
pred
test.err[nvariables] = with(boston[-train,], mean( (medv-pred)^2 ))
test.err
oob.err = double(ncol(boston)-1)
oob.err = double(ncol(boston)-1)
test.err = double(ncol(boston)-1)
for(nvariables in 1:(ncol(boston)-1)){
print(paste(Sys.time(),nvariables))
fit = randomForest(medv~., data = boston, subset=train, mtry=nvariables, ntree = 350)
oob.err[nvariables] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[nvariables] = with(boston[-train,], mean( (medv-pred)^2 ))
}
oob.err = double(ncol(boston)-1)
test.err = double(ncol(boston)-1)
for(nvariables in 1:(ncol(boston)-1)){
print(paste(Sys.time(),"Entrenando random Forest con",nvariables,"arboles"))
fit = randomForest(medv~., data = boston, subset=train, mtry=nvariables, ntree = 350)
print(paste(Sys.time(),"Fin: Entrenando random Forest con",nvariables,"arboles"))
oob.err[nvariables] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[nvariables] = with(boston[-train,], mean( (medv-pred)^2 ))
}
oob.err = double(ncol(boston)-1)
test.err = double(ncol(boston)-1)
for(nvariables in 1:(ncol(boston)-1)){
print(paste(Sys.time(),"Entrenando random Forest con",nvariables,"variables"))
fit = randomForest(medv~., data = boston, subset=train, mtry=nvariables, ntree = 350)
print(paste(Sys.time(),"Fin: Entrenando random Forest con",nvariables,"variables"))
oob.err[nvariables] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[nvariables] = with(boston[-train,], mean( (medv-pred)^2 ))
}
oob.err
test.err
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
matplot(1:nvariables, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
par(mfrow=c(1,1))
matplot(1:nvariables, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
matplot(1:nvariables, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y)
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y)
plot(x,y,col="red")
plot(x,y,col="red",pch=19)
lines(mean(x),col="blue")
lines(mean(y),col="blue")
mean(y)
lines(seq(1000,min(x),max(x)),mean(y),col="blue")
lines(seq(min(x),max(x),1/1000),mean(y),col="blue")
seq(min(x),max(x),1/1000)
my_x<-seq(min(x),max(x),1/1000)
my_x<-seq(1000,min(x),max(x))
my_x<-seq(min(x),max(x),1/1000)
my_x<-seq(min(x),max(x),1000)
my_x<-seq(min(x),max(x),1/2)
my_x<-seq(min(x),max(x),1/10)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(my_x,mean(y)),col="blue")
lines(my_x,rep(mean(y),my_x),col="blue")
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),my_x),col="blue")
rep(mean(y),my_x)
lines(my_x,rep(my_x,mean(y)),col="blue")
rep(2,3)
rep(mean(y),length(my_x))
lines(my_x,rep(mean(y),length(my_x)),col="blue")
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
my_ml<-lm(y~x)
lines(my_ml,col="blue",lwd=2)
my_ml<-lm(y~x)
lines(my_ml,col="blue",lwd=2)
plot(x,y,col="red",pch=19)
my_ml<-lm(y~x)
lines(my_ml,col="blue",lwd=2)
abline(my_ml,col="blue",lwd=2)
abline(my_ml,col="blue",lwd=1)
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
my_ml<-lm(y~x)
abline(my_ml,col="blue",lwd=1)
subplot(par(mfrow=c(1,2)))
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
par(mfrow=c(1,2))
plot(x,y,col="red",pch=19)
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
plot(x,y,col="red",pch=19)
my_ml<-lm(y~x)
abline(my_ml,col="blue",lwd=1)
par(mfrow=c(1,2))
plot(x,y,col="red",pch=19,main="var y")
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
plot(x,y,col="red",pch=19,main="lm")
my_ml<-lm(y~x)
abline(my_ml,col="blue",lwd=1)
summar(my_ml)
summary(my_ml)
x<-runif(4,25,50)
y<-2*x+runif(4,0,10)
par(mfrow=c(1,2))
plot(x,y,col="red",pch=19,main="var y")
my_x<-seq(min(x),max(x),1/100)
lines(my_x,rep(mean(y),length(my_x)),col="blue")
plot(x,y,col="red",pch=19,main="lm")
my_ml<-lm(y~x)
abline(my_ml,col="blue",lwd=1)
par(mfrow=c(1,1))
x<--1000:1000
y<-1/(1+exp(-x))
plot(x,y)
x<-seq(-3,3,1/10)
y<-1/(1+exp(-x))
plot(x,y)
plot(x,y,type="l")
x<-seq(-10,10,1/10)
y<-1/(1+exp(-x))
plot(x,y,type="l")
lines(x,rep(0.5,length(x)))
x<-seq(-10,10,1/10)
y<-1/(1+exp(-x))
plot(x,y,type="l",main="Logistic Function (sigmoide)",col="blue")
lines(x,rep(0.5,length(x)),col="red")
x<-seq(-10,10,1/10)
y<-1/(1+exp(-x))
plot(x,y,type="l",main="Logistic Function (sigmoide)",col="blue",lwd=2)
lines(x,rep(0.5,length(x)),col="red")
#Borramos los datos
rm(list=ls())
library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=500)
rf.boston
#de donde viene variance?
predicted=rf.boston$predicted
y=boston$medv[train]
1 - sum((y-predicted)^2)/sum((y-mean(y))^2)
plot(rf.boston)
#vamos a intentar para cada valor de mtry posible
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350] #por que elijo aqui 350?
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
plot(rf.boston)
#Borramos los datos
rm(list=ls())
#-------------- Pregunta 1: Lectura de datos ----------#
library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=500)
rf.boston
#como se calcula el % varianza explicada?
predicted=rf.boston$predicted
y=boston$medv[train]
1 - sum((y-predicted)^2)/sum((y-mean(y))^2) #=R^2
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston)
#vamos a intentar para cada valor de mtry posible
oob.err = double(13)
test.err = double(13)
?randomForest
#vamos a intentar para cada valor de mtry posible
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350] #por que elijo aqui 350?
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
#Borramos los datos
rm(list=ls())
library(MASS)
data(package="MASS")
#data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=500)
rf.boston
#como se calcula el % varianza explicada?
predicted=rf.boston$predicted
y=boston$medv[train]
1 - sum((y-predicted)^2)/sum((y-mean(y))^2) #=R^2
plot(rf.boston)
#vamos a intentar para cada valor de mtry posible
oob.err = double(13)
test.err = double(13)
boston
#vamos a intentar para cada valor de mtry posible
oob.err = double(13)
test.err = double(13)
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
mtry=1
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
fit
fit$mse[350]
fit$mse
pred = predict(fit, boston[-train,])
pred
#vamos a intentar para cada valor de mtry posible
oob.err = double(13) #out of bag error, cuantas variables dejo sin introducir en el bosque
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350] #por que elijo aqui solo el ultimo valor (350)?
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
if (!require("MASS")){install.packages("MASS",verbose = F) ; library("MASS")}
if (!require("randomForest")){install.packages("randomForest",verbose = F) ; library("randomForest")}
if (!require("gbm")){install.packages("gbm",verbose = F) ; library("gbm")}
#Borramos los datos
rm(list=ls())
library(MASS)
#Borramos los datos
rm(list=ls())
#-------------- Pregunta 1: Lectura de datos ----------#
library(MASS)
#data(package="MASS")
boston<-Boston
boston
boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
train
nrow(boston)
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=500)
rf.boston
#como se calcula el % varianza explicada?
predicted=rf.boston$predicted
y=boston$medv[train]
1 - sum((y-predicted)^2)/sum((y-mean(y))^2) #=R^2
plot(rf.boston)
plot(rf.boston,type="b")
plot(rf.boston)
#vamos a intentar para cada valor de mtry posible
oob.err = double(13) #out of bag error, cuantas variables dejo sin introducir en el bosque
test.err = double(13)
#vamos a intentar para cada valor de mtry posible
oob.err = double(13) #out of bag error, cuantas variables dejo sin introducir en el bosque
test.err = double(13)
mtry=1
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
fit$mse
#vamos a intentar para cada valor de mtry posible
oob.err = double(13) #out of bag error, cuantas variables dejo sin introducir en el bosque
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350] #por que elijo aqui solo el ultimo valor (350)? #porque es el error de los 350 arboles
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
test.err
oob.err
plot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
plot(1:mtry, test.err, pch = 23, col = "red", type = "b", ylab="Mean Squared Error")
lines(1:mtry, test.err, pch = 23, col = "blue", type = "b")
plot(1:mtry, test.err, pch = 23, col = "red", type = "b", ylab="Mean Squared Error")
lines(1:mtry, test.err, pch = 23, col = "blue", type = "b")
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
#Borramos los datos
rm(list=ls())
#-------------- Pregunta 1: Lectura de datos ----------#
library(MASS)
#data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
#-------------- Pregunta 2: datos de train ----------#
set.seed(101)
train = sample(1:nrow(boston), 300) #seleccionamos 300 valores para entrenar
#-------------- Pregunta 3: Ajuste del modelo ----------#
rf.boston = randomForest(medv~., data = boston, subset = train,ntree=500)
rf.boston
#como se calcula el % varianza explicada?
predicted=rf.boston$predicted
y=boston$medv[train]
1 - sum((y-predicted)^2)/sum((y-mean(y))^2) #=R^2
#-------------- Pregunta 4: Arboles vs. error ----------#
plot(rf.boston)
#-------------- Pregunta 5: oob error vs test error ----------#
#vamos a intentar para cada valor de mtry posible
oob.err = double(13) #out of bag error, cuantas variables dejo sin introducir en el bosque
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350] #por que elijo aqui solo el ultimo valor (350)?
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
#-------------- Pregunta 6: Grafico oob error vs test error ----------#
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
