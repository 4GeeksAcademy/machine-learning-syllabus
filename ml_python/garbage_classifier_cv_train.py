# -*- coding: utf-8 -*-
"""garbage_classifier_cv_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Atkt6OU8teVIdy3xxX7JPvnasbGC_wz

# All General Imports
"""

import warnings
import numpy as np
import os
import platform
import timeit

"""# All Functions Definitions"""

def data_download(file_to_download, gdrive_code, OS):
  if not os.path.exists(file_to_download):
    os.system('gdown --id "'+gdrive_code+'" --output '+file_to_download)
    if OS == "Linux":
        os.system('unzip -o -n "./'+file_to_download+'" -d "./"')

"""# All Downloads"""

start_time = timeit.default_timer()
# Operating System
OS = platform.system()                           # returns 'Windows', 'Linux', etc

os.system('pip install --upgrade --no-cache-dir gdown')

data_download("./Garbage_Data_Set.zip", "1e0RJH5HLdRVmPnDaamQLn0WVYFz4gOrE", OS)

data_download("./Garbage_Libs.zip", "1ZoMU2QGYLKhIaGZS8y4IYvmHofeqvAMz", OS)

print("Elapsed Time: ", timeit.default_timer() - start_time)

"""# All Imports

Keras imports and Program Imports
"""

from tensorflow.keras.datasets import cifar10
from tensorflow.keras.optimizers import SGD, Adam 
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau

from Preprocessing.preprocessing import LoadData
from Models.convnet import Lenet5
from Utils.view import View

"""# Initializations

Initialization of Globals, Hyperparameters, Dataset Parameters and Normalization
"""

with warnings.catch_warnings():
    warnings.filterwarnings("ignore", category=Warning)

default_callbacks = []

labels_garbage_list = ["glass", "metal", "paper", "plastic", "trash"]

learning_rate = 0.001
epochs = 100
limit = None
batch_size = 32
normalization = True
data_augmentation = False
save_best_model = True
early_stopping = False
log = True
R_LR_P = False

cifar10_ds = False
garbage_ds = True
develop = False
dest_input_size = 32

if develop: 
    limit = 1000
    epochs = 10

if cifar10_ds: 
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    input_size = x_train.shape[1]
    depth = x_train.shape[3]
    classes = len(np.unique(y_train))
    
if garbage_ds:
    [x_train, y_train, x_val, y_val, x_test, y_test] = LoadData.load_dataset_cv('./Garbage_Data_Set', labels_garbage_list, 0.1, 0.1, True, limit, dest_input_size, True)
    input_size = x_train.shape[1]
    depth = x_train.shape[3]
    classes = len(np.unique(y_train))

if cifar10_ds and limit is not None: 
    x_train = x_train[0:limit]
    y_train = y_train[0:limit]
    x_test = x_test[0:limit]
    y_test = y_test[0:limit]
    
if normalization: 
    x_train = x_train / 255.0
    x_test = x_test / 255.0

print(x_train.shape)
print(y_train.shape)
print(x_val.shape)
print(y_val.shape)
print(x_test.shape)
print(y_test.shape)

"""# Model Compiling

One-hot encoding of the output
"""

if cifar10_ds or garbage_ds: 
    y_train = to_categorical(y_train, num_classes = classes)
    y_val = to_categorical(y_val, num_classes = classes)
    y_test = to_categorical(y_test, num_classes = classes)

model = Lenet5.build(depth, input_size, input_size, classes, True)
#sgd = SGD(lr = learning_rate)
sgd = Adam(lr = learning_rate)

model.compile(optimizer = sgd, loss = "categorical_crossentropy", metrics = ["accuracy"])

"""# Callback Definitions

Save Best Model, Early Stopping, Csv Logger and Reduce On Plateau
"""

if save_best_model: 
  check_point = ModelCheckpoint("best_epoch_model.hdf5", save_weights_only = True, monitor = "val_accuracy", verbose = 1, save_best_only = True, mode = "max")
  default_callbacks = default_callbacks + [check_point]

if early_stopping: 
  earlyStopping = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, patience = 10, verbose = 0, mode = 'min') 
  default_callbacks = default_callbacks + [earlyStopping]

if log: 
  csv_logger = CSVLogger('history.log')
  default_callbacks = default_callbacks + [csv_logger]

if R_LR_P: 
  reduce_lr_plateau = ReduceLROnPlateau(factor=0.1, patience=3, verbose=1, min_lr=1e-06, min_delta=0.01)
  default_callbacks = default_callbacks + [reduce_lr_plateau]

"""# Model Training

Train the model, iterating on the data in batches
"""

if data_augmentation == False: 
	history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val), callbacks = default_callbacks, verbose = 2)
else:
	datagen = ImageDataGenerator(zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, rotation_range = 30)                                   

	history = model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size), steps_per_epoch = x_train.shape[0]/batch_size, epochs = epochs, validation_data = (x_val, y_val), callbacks = default_callbacks, verbose = 2)

score = model.evaluate(x_test, y_test, batch_size=64)
print(score)

model.save_weights('last_epoch_model.hdf5')
    
View.plot_loss(history)
View.plot_acc(history)

"""# Homeworks

1) Implement the Garbage Classifier with advanced models that we saw last time and make practice in order to find the best model.

2) Make some exploratory analysis on Input Data.

3) Make some exploratory analysis on Output Data.

4) Find the best hyperparameters for Data Augmentation.

5) Try to manually fine-tune all hyper-parameters in order to achieve the best accuracy on validation set

6) Save the best model and write an Inference code, where you give one external garbage image (downloaded from google, etc.) and classify it with the best model.
"""

