# -*- coding: utf-8 -*-
"""numpy_pandas_introduction_day_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dfndzYnBLpQ35g2vkoG82ZGw6tuxBXtv

# Introduction to Data Analysis with Python


<img src="https://www.python.org/static/img/python-logo.png" alt="yogen" style="width: 200px; float: right;"/>
<br>
<br>
<br>

## `numpy`

Base N-dimensional array package. We can use it to perform array operations. Its arrays are typed and can have any shape and dimension we want.

### Indexing

![Python slicing](https://infohost.nmt.edu/tcc/help/pubs/python/web/fig/slicing.png)
"""

print(a)
print(a2)

print(a[2],a2[0])

a2[1][0]

a2[1,0]

"""### Slicing"""

a

a[:4]

a2[:]

a2[:2,:2]

1>3

a2 > 3

cond = a2>3

a2[cond]

a2[cond & (a2 % 2 == 0)]

evens = a2 % 2 == 0
gt_3 = a2 > 3
even_and_gt_3 = evens & gt_3

a2[evens]

a2[gt_3]

a2[even_and_gt_3]

a

condA=[True,False,False,True,True,False]

a[condA]

"""### Careful with copying!"""

b = a2[1]

b

b[1] = -1

b

a2







b=a2[1]

b[1]=-1
a2

b[:]=3

a2

c=a2[1].copy()
c

c[:]=[-1000,-1000,-1000]

print(c)
print(a2)

"""### Element wise operations"""

print(a2)
a2*a2

a2 -a2

1/a2

a2**2

np.sin(a2)

"""### Matrix operations"""

d = np.array([1.0,1.0,2.0,3.0,0])

1+1+4+9

d.dot(d)

a.dot(a)

a2.dot(np.arange(3))

a2

a2.transpose()

a2.T

print(a2.sum())
print(a2.cumsum())
print(a2.cumprod())

"""### `ndarray` vs matrix"""

a3 = np.mat(a2)
a3

a3.T

a2*a2

a3.T*a3

a3[:2,:2].I

a3[:2,:2].I*a3[:2,:2]

a3[:2,:2].det

"""### Linear Algebra

http://docs.scipy.org/doc/numpy-1.10.0/reference/routines.linalg.html
"""

from numpy import linalg as la

help(la)

la.det(a3[:2,:2])

np.linalg.det(a3[:2,:2])

"""### Trace, determinant and inverse"""

np.trace(np.eye(3))

print(a2)
print(np.trace(a2))
print(np.trace(a2,offset=1))
print(np.trace(a2,offset=-1))

a = np.array([[1, 2], [3, 4]])
print(a)
print(a.shape)
la.det(a)

a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
a

print(a.shape)
print(la.det(a))

la.inv(np.array([[1,2],[3,4]]))

la.inv(np.array([[1,2],[3,4]])).dot(np.array([[1,2],[3,4]]))

"""## Exercise 1

In a chicken and rabbit farm, there are 35 heads and 94 legs. How many chickens and how many rabbits do we have?

Remember:

$$A \cdot X = b$$

$$A^{-1} \cdot A \cdot X = I \cdot X = A^{-1} \cdot b$$

$$X = A^{-1} \cdot b$$
"""

a2

A = np.array([[1,1],[2,4]])

b = np.array([35,94])

np.linalg.solve(A,b)

X = np.linalg.inv(A).dot(b)
X

"""### Norm of a vector and a matrix

We can also calculate norms- the order 2 norm is the Euclidean distance, ie the length of a vector
"""

la.norm(np.arange(3))

math.sqrt(0**2 + 1 ** 2 + 2 ** 2)

a = np.array([[1, 2], [3, 4]])
help(la.norm)

print(a)
print(la.norm(a))
print(la.norm(a,2))
print(la.norm(a,'fro'))
print(la.norm(a.reshape(4),2))
print(la.norm(a,1))
print(la.norm(a,np.inf))

b = np.arange(1,4)
print(b)
print(la.norm(b))
print(la.norm(b,2))
print(la.norm(b,1))
print(la.norm(b,np.inf))

"""#### Exercise

In a chicken and rabbit farm, there are 35 heads and 94 legs. How many chickens and how many rabbits do we have?

Remember: for every number $n$

$$A \cdot X = B$$

$$A^{-1} \cdot A \cdot X = I \cdot X = A^{-1} \cdot B$$

$$X = A^{-1} \cdot B$$

\* The language I've used to represent this formula is [$\LaTeX$](https://www.latex-project.org/). It's used to typeset all kinds of things from cvs to scientific articles to books. You can find a quick introduction [here](https://www.cs.princeton.edu/courses/archive/spr10/cos433/Latex/latex-guide.pdf). Almost everything you need to know to write equations in the notebook is on pages 4,5 and 6 of that pdf.

Example:

$A^x A_i A^-1$
"""

A = np.mat([[2,4], [1,1]])
B = np.mat([94,35]).T

X = A.I.dot(B)
X

"""## Exercise 2: 

A Linear Regression example with numpy

Now, we are ready to implement our own linear regression example.

In linear regression, our hypothesis function $h_\theta$ is:

$$h_\theta(x) = \theta_0 + \theta_1x$$

And, as we are doing regression, our cost function is: 

$$J(\theta_0,\theta_1) = \frac{1}{m}\sum_{i=1}^m(\hat{y}_i-y_i)^2 = \frac{1}{m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2 $$

### Generate dummy data
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

theta_0 = 2
theta_1 = 5

X = (np.random.randn(100) + 1) * 50
jitter = 50 * np.random.randn(100)
Y = theta_0 + theta_1 * X + jitter

plt.scatter (X, Y)

"""That is the spread that we will try to aproximate with our line.

### Write the cost function
"""

def cost_function(X, Y):
    
    return lambda thetas: sum((thetas[0] + thetas[1] * X - Y) ** 2) / len(X)

J = cost_function(X, Y)
J([theta_0,theta_1])

from scipy.optimize import fmin

fmin(J, [0,0])

"""Gradient descent

Remember, we have to descend in the direction of the steepest gradient. For that, we need to now what direction the gradient points!

## Exercise 3:

Partial differentials of the cost function

$$\frac{\partial}{\partial\theta_0} = \frac{1}{m}\sum_{i=1}^m(h_\theta(x_i)-y_i)$$

$$\frac{\partial}{\partial\theta_1} = \frac{1}{m}\sum_{i=1}^m(h_\theta(x_i)-y_i) \cdot x_i$$
"""

def derivative_theta_0(X, Y):
    return lambda theta_0, theta_1: sum(theta_0 + theta_1 * X - Y) / len(X)

def derivative_theta_1(X, Y):
    return lambda theta_0, theta_1: sum((theta_0 + theta_1 * X - Y) * X) / len(X)

J_prime_0 = derivative_theta_0(X, Y)
J_prime_1 = derivative_theta_1(X, Y)

"""# Esercizio 4

Make element-wise product and dot product without numpy in Python

# Exercise 5

Write python code to make the activation (or "spread") of an Artificial Neural Network. Given: 

***b*** the vector of biases 

***W*** the matrix of Synaptical Weiths

***X*** the row vector of inputs

*hint: * adopt the sigmoid function as activation function
"""

X = X.astype(float)
O = sigmoid(np.dot(X, W))

"""# Homework: generate a random W matrix for a neural network and feed it with previous home work mnist data in order to activate a neural network

# Additional References

[Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do)

[What is SciPy?](https://www.scipy.org/)

[How can SciPy be fast if it is written in an interpreted language like Python?](https://www.scipy.org/scipylib/faq.html#how-can-scipy-be-fast-if-it-is-written-in-an-interpreted-language-like-python)

[What is the difference between NumPy and SciPy?](https://www.scipy.org/scipylib/faq.html#what-is-the-difference-between-numpy-and-scipy)

[Linear Algebra for AI](https://github.com/fastai/fastai/blob/master/tutorials/linalg_pytorch.ipynb)
"""